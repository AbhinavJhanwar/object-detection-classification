{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 10:24:08.983168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 10:24:09.962320: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-06 10:24:09.962357: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-06 10:24:12.016664: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 10:24:12.016767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 10:24:12.016783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.engine.topology'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2482956/3924201012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroi_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/object-detection-classification/frcnn/models/vgg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine.topology'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "import configparser\n",
    "import math\n",
    "from data_generator import data_generators\n",
    "from models import vgg as nn\n",
    "from models import losses as model_losses\n",
    "from models import roi_helpers\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('training_config.conf')\n",
    "model_params = {}\n",
    "    \n",
    "verbose = eval(config['model_params']['verbose'])\n",
    "\n",
    "# setting for data augmentation\n",
    "use_horizontal_flips = eval(config['model_params']['use_horizontal_flips'])\n",
    "use_vertical_flips = eval(config['model_params']['use_vertical_flips'])\n",
    "rot_90 = eval(config['model_params']['rot_90'])\n",
    "\n",
    "# anchor box scales\n",
    "# Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "# Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "model_params['anchor_box_scales'] = eval(config['model_params']['anchor_box_scales'])\n",
    "\n",
    "# anchor box ratios\n",
    "model_params['anchor_box_ratios'] = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "# size to resize the smallest side of the image\n",
    "# Original setting in paper is 600. Set to 416 in here to save training time\n",
    "model_params['im_size'] = eval(config['model_params']['im_size'])\n",
    "\n",
    "# image channel-wise mean to subtract\n",
    "model_params['img_channel_mean'] = eval(config['model_params']['img_channel_mean'])\n",
    "model_params['img_scaling_factor'] = eval(config['model_params']['img_scaling_factor'])\n",
    "\n",
    "# number of ROIs to process at once\n",
    "num_rois = eval(config['model_params']['num_rois'])\n",
    "\n",
    "# stride at the RPN (this depends on the network configuration)\n",
    "model_params['rpn_stride'] = eval(config['model_params']['rpn_stride'])\n",
    "\n",
    "balanced_classes = False\n",
    "\n",
    "# scaling the stdev\n",
    "model_params['std_scaling'] = eval(config['model_params']['std_scaling'])\n",
    "model_params['classifier_regr_std'] = eval(config['model_params']['classifier_regr_std'])\n",
    "\n",
    "# overlaps for RPN\n",
    "model_params['rpn_min_overlap'] = eval(config['model_params']['rpn_min_overlap'])\n",
    "model_params['rpn_max_overlap'] = eval(config['model_params']['rpn_max_overlap'])\n",
    "\n",
    "# overlaps for classifier ROIs\n",
    "model_params['classifier_min_overlap'] = eval(config['model_params']['classifier_min_overlap'])\n",
    "model_params['classifier_max_overlap'] = eval(config['model_params']['classifier_max_overlap'])\n",
    "\n",
    "# placeholder for the class mapping, automatically generated by the parser\n",
    "class_mapping = None\n",
    "\n",
    "model_path = eval(config['outputs']['model_path'])\n",
    "model_path_regex = re.match(\"^(.+)(\\.hdf5)$\", model_path)\n",
    "if model_path_regex.group(2) != '.hdf5':\n",
    "\tprint('Output weights must have .hdf5 filetype')\n",
    "\texit(1)\n",
    "\n",
    "# Input path for weights. If not specified, will try to load default weights provided by keras.\n",
    "input_weight_path = eval(config['inputs']['input_weight_path'])\n",
    "\n",
    "# Location to store all the metadata related to the training (to be used when testing).\n",
    "class_mapping_filename = eval(config['outputs']['class_mapping'])\n",
    "\n",
    "# get data option out of txt or xml\n",
    "data_options = eval(config['inputs']['data_options'])\n",
    "\n",
    "num_epochs = eval(config['model_params']['num_epochs'])\n",
    "\n",
    "record_path = 'record.csv' # Record data (used to save the losses, classification accuracy and mean average precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if weight path was passed via command line\n",
    "if input_weight_path:\n",
    "\tbase_net_weights = input_weight_path\n",
    "else:\n",
    "\t# set the path to weights based on backend and model\n",
    "\tbase_net_weights = nn.get_weight_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "Parsing annotation files\n",
      "Parsing annotation files\n",
      "Training images per class:\n",
      "{'Accordion': 192,\n",
      " 'Adhesive tape': 49,\n",
      " 'Aircraft': 306,\n",
      " 'Airplane': 2658,\n",
      " 'Alarm clock': 32,\n",
      " 'Alpaca': 111,\n",
      " 'Ambulance': 99,\n",
      " 'Animal': 1510,\n",
      " 'Ant': 135,\n",
      " 'Antelope': 368,\n",
      " 'Apple': 771,\n",
      " 'Armadillo': 6,\n",
      " 'Artichoke': 46,\n",
      " 'Asparagus': 66,\n",
      " 'Auto part': 909,\n",
      " 'Axe': 27,\n",
      " 'Backpack': 444,\n",
      " 'Bagel': 139,\n",
      " 'Baked goods': 2334,\n",
      " 'Balance beam': 52,\n",
      " 'Ball': 859,\n",
      " 'Balloon': 1788,\n",
      " 'Banana': 342,\n",
      " 'Band-aid': 6,\n",
      " 'Banjo': 35,\n",
      " 'Barge': 114,\n",
      " 'Barrel': 405,\n",
      " 'Baseball bat': 275,\n",
      " 'Baseball glove': 500,\n",
      " 'Bat': 140,\n",
      " 'Bathroom accessory': 206,\n",
      " 'Bathroom cabinet': 100,\n",
      " 'Bathtub': 134,\n",
      " 'Beaker': 31,\n",
      " 'Bear': 94,\n",
      " 'Bed': 609,\n",
      " 'Bee': 1689,\n",
      " 'Beehive': 103,\n",
      " 'Beer': 1177,\n",
      " 'Beetle': 411,\n",
      " 'Bell pepper': 192,\n",
      " 'Belt': 57,\n",
      " 'Bench': 1051,\n",
      " 'Bicycle': 4271,\n",
      " 'Bicycle helmet': 2607,\n",
      " 'Bicycle wheel': 6624,\n",
      " 'Bidet': 130,\n",
      " 'Billboard': 1253,\n",
      " 'Billiard table': 169,\n",
      " 'Binoculars': 26,\n",
      " 'Bird': 4730,\n",
      " 'Blender': 59,\n",
      " 'Blue jay': 39,\n",
      " 'Boat': 8330,\n",
      " 'Bomb': 8,\n",
      " 'Book': 4960,\n",
      " 'Bookcase': 885,\n",
      " 'Boot': 581,\n",
      " 'Bottle': 5779,\n",
      " 'Bottle opener': 5,\n",
      " 'Bow and arrow': 89,\n",
      " 'Bowl': 737,\n",
      " 'Bowling equipment': 544,\n",
      " 'Box': 802,\n",
      " 'Boy': 7994,\n",
      " 'Brassiere': 415,\n",
      " 'Bread': 868,\n",
      " 'Briefcase': 42,\n",
      " 'Broccoli': 229,\n",
      " 'Bronze sculpture': 496,\n",
      " 'Brown bear': 124,\n",
      " 'Building': 15686,\n",
      " 'Bull': 315,\n",
      " 'Burrito': 43,\n",
      " 'Bus': 1577,\n",
      " 'Bust': 215,\n",
      " 'Butterfly': 1743,\n",
      " 'Cabbage': 104,\n",
      " 'Cabinetry': 1628,\n",
      " 'Cake': 825,\n",
      " 'Cake stand': 134,\n",
      " 'Calculator': 34,\n",
      " 'Camel': 232,\n",
      " 'Camera': 965,\n",
      " 'Can opener': 1,\n",
      " 'Canary': 57,\n",
      " 'Candle': 1372,\n",
      " 'Candy': 487,\n",
      " 'Cannon': 239,\n",
      " 'Canoe': 551,\n",
      " 'Cantaloupe': 14,\n",
      " 'Car': 22667,\n",
      " 'Carnivore': 461,\n",
      " 'Carrot': 299,\n",
      " 'Cart': 418,\n",
      " 'Cassette deck': 11,\n",
      " 'Castle': 597,\n",
      " 'Cat': 1808,\n",
      " 'Cat furniture': 47,\n",
      " 'Caterpillar': 183,\n",
      " 'Cattle': 2071,\n",
      " 'Ceiling fan': 105,\n",
      " 'Cello': 195,\n",
      " 'Centipede': 73,\n",
      " 'Chainsaw': 11,\n",
      " 'Chair': 13469,\n",
      " 'Cheese': 377,\n",
      " 'Cheetah': 115,\n",
      " 'Chest of drawers': 311,\n",
      " 'Chicken': 814,\n",
      " 'Chime': 5,\n",
      " 'Chisel': 7,\n",
      " 'Chopsticks': 128,\n",
      " 'Christmas tree': 710,\n",
      " 'Clock': 238,\n",
      " 'Closet': 170,\n",
      " 'Clothing': 124959,\n",
      " 'Coat': 1103,\n",
      " 'Cocktail': 625,\n",
      " 'Cocktail shaker': 11,\n",
      " 'Coconut': 194,\n",
      " 'Coffee': 478,\n",
      " 'Coffee cup': 786,\n",
      " 'Coffee table': 708,\n",
      " 'Coffeemaker': 52,\n",
      " 'Coin': 490,\n",
      " 'Common fig': 49,\n",
      " 'Computer keyboard': 908,\n",
      " 'Computer monitor': 717,\n",
      " 'Computer mouse': 163,\n",
      " 'Convenience store': 334,\n",
      " 'Cookie': 885,\n",
      " 'Cooking spray': 6,\n",
      " 'Corded phone': 79,\n",
      " 'Cosmetics': 625,\n",
      " 'Couch': 452,\n",
      " 'Countertop': 657,\n",
      " 'Cowboy hat': 421,\n",
      " 'Crab': 177,\n",
      " 'Cream': 44,\n",
      " 'Cricket ball': 10,\n",
      " 'Crocodile': 208,\n",
      " 'Croissant': 102,\n",
      " 'Crown': 153,\n",
      " 'Crutch': 24,\n",
      " 'Cucumber': 213,\n",
      " 'Cupboard': 340,\n",
      " 'Curtain': 884,\n",
      " 'Cutting board': 65,\n",
      " 'Dagger': 97,\n",
      " 'Dairy': 1222,\n",
      " 'Deer': 796,\n",
      " 'Desk': 1351,\n",
      " 'Dessert': 2760,\n",
      " 'Diaper': 43,\n",
      " 'Dice': 70,\n",
      " 'Digital clock': 74,\n",
      " 'Dinosaur': 380,\n",
      " 'Dishwasher': 23,\n",
      " 'Dog': 3085,\n",
      " 'Dog bed': 82,\n",
      " 'Doll': 984,\n",
      " 'Dolphin': 243,\n",
      " 'Door': 2138,\n",
      " 'Door handle': 153,\n",
      " 'Doughnut': 225,\n",
      " 'Dragonfly': 367,\n",
      " 'Drawer': 836,\n",
      " 'Dress': 5406,\n",
      " 'Drill': 24,\n",
      " 'Drink': 3968,\n",
      " 'Drinking straw': 78,\n",
      " 'Drum': 3387,\n",
      " 'Duck': 1888,\n",
      " 'Dumbbell': 54,\n",
      " 'Eagle': 276,\n",
      " 'Earrings': 334,\n",
      " 'Egg': 423,\n",
      " 'Elephant': 753,\n",
      " 'Envelope': 31,\n",
      " 'Eraser': 16,\n",
      " 'Face powder': 14,\n",
      " 'Facial tissue holder': 5,\n",
      " 'Falcon': 328,\n",
      " 'Fashion accessory': 8177,\n",
      " 'Fast food': 3058,\n",
      " 'Fax': 7,\n",
      " 'Fedora': 485,\n",
      " 'Filing cabinet': 92,\n",
      " 'Fire hydrant': 62,\n",
      " 'Fireplace': 220,\n",
      " 'Fish': 3205,\n",
      " 'Flag': 3597,\n",
      " 'Flashlight': 25,\n",
      " 'Flower': 33118,\n",
      " 'Flowerpot': 2849,\n",
      " 'Flute': 63,\n",
      " 'Flying disc': 56,\n",
      " 'Food': 7553,\n",
      " 'Food processor': 46,\n",
      " 'Football': 657,\n",
      " 'Football helmet': 2887,\n",
      " 'Footwear': 66530,\n",
      " 'Fork': 348,\n",
      " 'Fountain': 626,\n",
      " 'Fox': 88,\n",
      " 'French fries': 300,\n",
      " 'Frog': 394,\n",
      " 'Fruit': 2778,\n",
      " 'Frying pan': 104,\n",
      " 'Furniture': 3507,\n",
      " 'Gas stove': 147,\n",
      " 'Giraffe': 359,\n",
      " 'Girl': 20034,\n",
      " 'Glasses': 5490,\n",
      " 'Glove': 233,\n",
      " 'Goat': 536,\n",
      " 'Goggles': 1245,\n",
      " 'Goldfish': 628,\n",
      " 'Golf ball': 117,\n",
      " 'Golf cart': 76,\n",
      " 'Gondola': 398,\n",
      " 'Goose': 1077,\n",
      " 'Grape': 724,\n",
      " 'Grapefruit': 405,\n",
      " 'Grinder': 1,\n",
      " 'Guacamole': 40,\n",
      " 'Guitar': 2834,\n",
      " 'Hair dryer': 3,\n",
      " 'Hair spray': 4,\n",
      " 'Hamburger': 409,\n",
      " 'Hammer': 29,\n",
      " 'Hamster': 79,\n",
      " 'Hand dryer': 14,\n",
      " 'Handbag': 376,\n",
      " 'Handgun': 136,\n",
      " 'Harbor seal': 358,\n",
      " 'Harmonica': 6,\n",
      " 'Harp': 29,\n",
      " 'Harpsichord': 38,\n",
      " 'Hat': 1360,\n",
      " 'Headphones': 242,\n",
      " 'Heater': 12,\n",
      " 'Hedgehog': 23,\n",
      " 'Helicopter': 695,\n",
      " 'Helmet': 1806,\n",
      " 'High heels': 594,\n",
      " 'Hiking equipment': 1088,\n",
      " 'Hippopotamus': 101,\n",
      " 'Home appliance': 311,\n",
      " 'Honeycomb': 59,\n",
      " 'Horizontal bar': 21,\n",
      " 'Horn': 135,\n",
      " 'Horse': 1934,\n",
      " 'Hot dog': 102,\n",
      " 'House': 13740,\n",
      " 'Houseplant': 2439,\n",
      " 'Human arm': 20703,\n",
      " 'Human beard': 712,\n",
      " 'Human body': 17725,\n",
      " 'Human ear': 2051,\n",
      " 'Human eye': 8074,\n",
      " 'Human face': 89269,\n",
      " 'Human foot': 871,\n",
      " 'Human hair': 23281,\n",
      " 'Human hand': 7925,\n",
      " 'Human head': 19870,\n",
      " 'Human leg': 8027,\n",
      " 'Human mouth': 4576,\n",
      " 'Human nose': 5989,\n",
      " 'Humidifier': 1,\n",
      " 'Ice cream': 549,\n",
      " 'Indoor rower': 2,\n",
      " 'Infant bed': 83,\n",
      " 'Insect': 973,\n",
      " 'Invertebrate': 215,\n",
      " 'Ipod': 99,\n",
      " 'Isopod': 23,\n",
      " 'Jacket': 2683,\n",
      " 'Jacuzzi': 14,\n",
      " 'Jaguar': 130,\n",
      " 'Jeans': 8133,\n",
      " 'Jellyfish': 584,\n",
      " 'Jet ski': 97,\n",
      " 'Jug': 141,\n",
      " 'Juice': 448,\n",
      " 'Kangaroo': 128,\n",
      " 'Kettle': 97,\n",
      " 'Kitchen & dining room table': 247,\n",
      " 'Kitchen appliance': 778,\n",
      " 'Kitchen knife': 72,\n",
      " 'Kitchen utensil': 140,\n",
      " 'Kite': 341,\n",
      " 'Knife': 140,\n",
      " 'Koala': 50,\n",
      " 'Ladder': 141,\n",
      " 'Ladle': 14,\n",
      " 'Ladybug': 133,\n",
      " 'Lamp': 598,\n",
      " 'Land vehicle': 7490,\n",
      " 'Lantern': 1152,\n",
      " 'Laptop': 1171,\n",
      " 'Lavender': 1345,\n",
      " 'Lemon': 334,\n",
      " 'Leopard': 165,\n",
      " 'Lifejacket': 636,\n",
      " 'Light bulb': 476,\n",
      " 'Light switch': 27,\n",
      " 'Lighthouse': 369,\n",
      " 'Lily': 432,\n",
      " 'Limousine': 59,\n",
      " 'Lion': 401,\n",
      " 'Lipstick': 337,\n",
      " 'Lizard': 414,\n",
      " 'Lobster': 119,\n",
      " 'Loveseat': 121,\n",
      " 'Luggage and bags': 382,\n",
      " 'Lynx': 22,\n",
      " 'Magpie': 18,\n",
      " 'Mammal': 15372,\n",
      " 'Man': 124019,\n",
      " 'Mango': 112,\n",
      " 'Maple': 457,\n",
      " 'Maracas': 2,\n",
      " 'Marine invertebrates': 1812,\n",
      " 'Marine mammal': 368,\n",
      " 'Measuring cup': 14,\n",
      " 'Mechanical fan': 132,\n",
      " 'Medical equipment': 393,\n",
      " 'Microphone': 2818,\n",
      " 'Microwave oven': 124,\n",
      " 'Milk': 44,\n",
      " 'Miniskirt': 231,\n",
      " 'Mirror': 265,\n",
      " 'Missile': 114,\n",
      " 'Mixer': 49,\n",
      " 'Mixing bowl': 234,\n",
      " 'Mobile phone': 1085,\n",
      " 'Monkey': 625,\n",
      " 'Moths and butterflies': 286,\n",
      " 'Motorcycle': 1888,\n",
      " 'Mouse': 158,\n",
      " 'Muffin': 955,\n",
      " 'Mug': 392,\n",
      " 'Mule': 212,\n",
      " 'Mushroom': 942,\n",
      " 'Musical instrument': 1640,\n",
      " 'Musical keyboard': 343,\n",
      " 'Nail': 130,\n",
      " 'Necklace': 462,\n",
      " 'Nightstand': 283,\n",
      " 'Oboe': 25,\n",
      " 'Office building': 630,\n",
      " 'Office supplies': 1014,\n",
      " 'Orange': 1136,\n",
      " 'Organ': 89,\n",
      " 'Ostrich': 78,\n",
      " 'Otter': 99,\n",
      " 'Oven': 176,\n",
      " 'Owl': 395,\n",
      " 'Oyster': 125,\n",
      " 'Paddle': 1487,\n",
      " 'Palm tree': 4145,\n",
      " 'Pancake': 153,\n",
      " 'Panda': 181,\n",
      " 'Paper towel': 39,\n",
      " 'Parachute': 573,\n",
      " 'Parking meter': 24,\n",
      " 'Parrot': 544,\n",
      " 'Pasta': 231,\n",
      " 'Pastry': 1223,\n",
      " 'Peach': 151,\n",
      " 'Pear': 142,\n",
      " 'Pen': 340,\n",
      " 'Pencil case': 46,\n",
      " 'Pencil sharpener': 5,\n",
      " 'Penguin': 911,\n",
      " 'Perfume': 59,\n",
      " 'Person': 90930,\n",
      " 'Personal care': 149,\n",
      " 'Piano': 287,\n",
      " 'Picnic basket': 87,\n",
      " 'Picture frame': 1433,\n",
      " 'Pig': 349,\n",
      " 'Pillow': 736,\n",
      " 'Pineapple': 120,\n",
      " 'Pitcher': 62,\n",
      " 'Pizza': 422,\n",
      " 'Pizza cutter': 1,\n",
      " 'Plant': 22859,\n",
      " 'Plastic bag': 236,\n",
      " 'Plate': 1062,\n",
      " 'Platter': 761,\n",
      " 'Plumbing fixture': 75,\n",
      " 'Polar bear': 115,\n",
      " 'Pomegranate': 128,\n",
      " 'Popcorn': 45,\n",
      " 'Porch': 462,\n",
      " 'Porcupine': 33,\n",
      " 'Poster': 2539,\n",
      " 'Potato': 153,\n",
      " 'Power plugs and sockets': 71,\n",
      " 'Pressure cooker': 1,\n",
      " 'Pretzel': 69,\n",
      " 'Printer': 40,\n",
      " 'Pumpkin': 1457,\n",
      " 'Punching bag': 50,\n",
      " 'Rabbit': 335,\n",
      " 'Raccoon': 42,\n",
      " 'Racket': 45,\n",
      " 'Radish': 123,\n",
      " 'Ratchet': 35,\n",
      " 'Raven': 59,\n",
      " 'Rays and skates': 108,\n",
      " 'Red panda': 55,\n",
      " 'Refrigerator': 128,\n",
      " 'Remote control': 40,\n",
      " 'Reptile': 72,\n",
      " 'Rhinoceros': 129,\n",
      " 'Rifle': 524,\n",
      " 'Ring binder': 27,\n",
      " 'Rocket': 208,\n",
      " 'Roller skates': 1212,\n",
      " 'Rose': 1401,\n",
      " 'Rugby ball': 88,\n",
      " 'Ruler': 50,\n",
      " 'Salad': 446,\n",
      " 'Salt and pepper shakers': 23,\n",
      " 'Sandal': 713,\n",
      " 'Sandwich': 222,\n",
      " 'Saucer': 521,\n",
      " 'Saxophone': 217,\n",
      " 'Scale': 25,\n",
      " 'Scarf': 389,\n",
      " 'Scissors': 67,\n",
      " 'Scoreboard': 118,\n",
      " 'Scorpion': 25,\n",
      " 'Screwdriver': 21,\n",
      " 'Sculpture': 3327,\n",
      " 'Sea lion': 346,\n",
      " 'Sea turtle': 128,\n",
      " 'Seafood': 530,\n",
      " 'Seahorse': 33,\n",
      " 'Seat belt': 138,\n",
      " 'Segway': 51,\n",
      " 'Serving tray': 48,\n",
      " 'Sewing machine': 76,\n",
      " 'Shark': 130,\n",
      " 'Sheep': 836,\n",
      " 'Shelf': 2109,\n",
      " 'Shellfish': 368,\n",
      " 'Shirt': 1120,\n",
      " 'Shorts': 2148,\n",
      " 'Shotgun': 226,\n",
      " 'Shower': 75,\n",
      " 'Shrimp': 301,\n",
      " 'Sink': 334,\n",
      " 'Skateboard': 362,\n",
      " 'Ski': 743,\n",
      " 'Skirt': 267,\n",
      " 'Skull': 473,\n",
      " 'Skunk': 5,\n",
      " 'Skyscraper': 8312,\n",
      " 'Slow cooker': 10,\n",
      " 'Snack': 4173,\n",
      " 'Snail': 131,\n",
      " 'Snake': 351,\n",
      " 'Snowboard': 186,\n",
      " 'Snowman': 187,\n",
      " 'Snowmobile': 45,\n",
      " 'Snowplow': 47,\n",
      " 'Soap dispenser': 11,\n",
      " 'Sock': 384,\n",
      " 'Sofa bed': 234,\n",
      " 'Sombrero': 97,\n",
      " 'Sparrow': 254,\n",
      " 'Spatula': 23,\n",
      " 'Spice rack': 23,\n",
      " 'Spider': 396,\n",
      " 'Spoon': 338,\n",
      " 'Sports equipment': 3710,\n",
      " 'Sports uniform': 2482,\n",
      " 'Squash': 38,\n",
      " 'Squid': 44,\n",
      " 'Squirrel': 395,\n",
      " 'Stairs': 768,\n",
      " 'Stapler': 5,\n",
      " 'Starfish': 119,\n",
      " 'Stationary bicycle': 47,\n",
      " 'Stethoscope': 23,\n",
      " 'Stool': 228,\n",
      " 'Stop sign': 62,\n",
      " 'Strawberry': 1652,\n",
      " 'Street light': 4935,\n",
      " 'Stretcher': 38,\n",
      " 'Studio couch': 353,\n",
      " 'Submarine': 10,\n",
      " 'Submarine sandwich': 61,\n",
      " 'Suit': 10787,\n",
      " 'Suitcase': 153,\n",
      " 'Sun hat': 1029,\n",
      " 'Sunflower': 1815,\n",
      " 'Sunglasses': 2618,\n",
      " 'Surfboard': 583,\n",
      " 'Sushi': 486,\n",
      " 'Swan': 996,\n",
      " 'Swim cap': 179,\n",
      " 'Swimming pool': 617,\n",
      " 'Swimwear': 1888,\n",
      " 'Sword': 152,\n",
      " 'Syringe': 17,\n",
      " 'Table': 8098,\n",
      " 'Table tennis racket': 162,\n",
      " 'Tablet computer': 183,\n",
      " 'Tableware': 3872,\n",
      " 'Taco': 153,\n",
      " 'Tank': 424,\n",
      " 'Tap': 363,\n",
      " 'Tart': 241,\n",
      " 'Taxi': 701,\n",
      " 'Tea': 241,\n",
      " 'Teapot': 101,\n",
      " 'Teddy bear': 300,\n",
      " 'Telephone': 37,\n",
      " 'Television': 540,\n",
      " 'Tennis ball': 124,\n",
      " 'Tennis racket': 247,\n",
      " 'Tent': 1196,\n",
      " 'Tiara': 96,\n",
      " 'Tick': 35,\n",
      " 'Tie': 1267,\n",
      " 'Tiger': 305,\n",
      " 'Tin can': 415,\n",
      " 'Tire': 11576,\n",
      " 'Toaster': 12,\n",
      " 'Toilet': 270,\n",
      " 'Toilet paper': 97,\n",
      " 'Tomato': 1349,\n",
      " 'Tool': 210,\n",
      " 'Toothbrush': 32,\n",
      " 'Torch': 5,\n",
      " 'Tortoise': 449,\n",
      " 'Towel': 68,\n",
      " 'Tower': 6941,\n",
      " 'Toy': 6871,\n",
      " 'Traffic light': 1442,\n",
      " 'Traffic sign': 843,\n",
      " 'Train': 1561,\n",
      " 'Training bench': 35,\n",
      " 'Treadmill': 62,\n",
      " 'Tree': 92642,\n",
      " 'Tree house': 23,\n",
      " 'Tripod': 301,\n",
      " 'Trombone': 107,\n",
      " 'Trousers': 1158,\n",
      " 'Truck': 1481,\n",
      " 'Trumpet': 206,\n",
      " 'Turkey': 94,\n",
      " 'Turtle': 78,\n",
      " 'Umbrella': 1096,\n",
      " 'Unicycle': 36,\n",
      " 'Van': 1042,\n",
      " 'Vase': 463,\n",
      " 'Vegetable': 1868,\n",
      " 'Vehicle': 4598,\n",
      " 'Vehicle registration plate': 1053,\n",
      " 'Violin': 575,\n",
      " 'Volleyball': 175,\n",
      " 'Waffle': 96,\n",
      " 'Waffle iron': 6,\n",
      " 'Wall clock': 224,\n",
      " 'Wardrobe': 56,\n",
      " 'Washing machine': 138,\n",
      " 'Waste container': 364,\n",
      " 'Watch': 432,\n",
      " 'Watercraft': 704,\n",
      " 'Watermelon': 197,\n",
      " 'Weapon': 265,\n",
      " 'Whale': 247,\n",
      " 'Wheel': 30386,\n",
      " 'Wheelchair': 287,\n",
      " 'Whisk': 32,\n",
      " 'Whiteboard': 202,\n",
      " 'Willow': 145,\n",
      " 'Window': 44612,\n",
      " 'Window blind': 141,\n",
      " 'Wine': 2135,\n",
      " 'Wine glass': 1971,\n",
      " 'Wine rack': 46,\n",
      " 'Winter melon': 16,\n",
      " 'Wok': 176,\n",
      " 'Woman': 67751,\n",
      " 'Wood-burning stove': 88,\n",
      " 'Woodpecker': 57,\n",
      " 'Worm': 48,\n",
      " 'Wrench': 27,\n",
      " 'Zebra': 235,\n",
      " 'Zucchini': 204,\n",
      " 'bg': 0}\n",
      "Num classes (including bg) = 599\n",
      "Class mapping has been written to class_mapping_open_images.pickle, and can be loaded when testing to ensure correct results\n"
     ]
    }
   ],
   "source": [
    "if data_options == 'xml':\n",
    "\t# training data path\n",
    "\tdata_path = eval(config['inputs']['data_path'])\n",
    "\tfrom data_generator.pascal_voc_parser import get_data\n",
    "\tall_imgs, classes_count, class_mapping = get_data(data_path)\n",
    "\n",
    "elif data_options == 'txt':\n",
    "\t# takes around 15-30 minutes to complete the task- depends on size of data\n",
    "\ttrain_data_path = eval(config['inputs']['train_annotation'])\n",
    "\tval_data_path = eval(config['inputs']['val_annotation'])\n",
    "\ttest_data_path = eval(config['inputs']['test_annotation'])\n",
    "\tfrom data_generator.simple_parser import get_data\n",
    "\tall_imgs1, classes_count, class_mapping = get_data(test_data_path)\n",
    "\tall_imgs2, classes_count, class_mapping = get_data(val_data_path)\n",
    "\tall_imgs, classes_count, class_mapping = get_data(train_data_path)\n",
    "\tall_imgs = all_imgs + all_imgs1 + all_imgs2\n",
    "\n",
    "else:\n",
    "\traise ValueError(\"data_options must be one of 'xml' or 'txt'\")\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "\tclasses_count['bg'] = 0\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "# inv_map for converting class index back to class\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print(f'Num classes (including bg) = {len(classes_count)}')\n",
    "\n",
    "# save class mapping\n",
    "with open(class_mapping_filename, 'wb') as f:\n",
    "\tpickle.dump(class_mapping, f)\n",
    "\tprint(f'Class mapping has been written to {class_mapping_filename}, and can be loaded when testing to ensure correct results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 04:52:26.510483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-02 04:52:27.157590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.157812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-12-02 04:52:27.157842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-02 04:52:27.159994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-02 04:52:27.160083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-12-02 04:52:27.161845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-02 04:52:27.162203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-02 04:52:27.164149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-12-02 04:52:27.164934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-12-02 04:52:27.170014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-02 04:52:27.170164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.170417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.170580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-12-02 04:52:27.171016: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 04:52:27.178749: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
      "2022-12-02 04:52:27.179075: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20ffdd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-02 04:52:27.179099: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-12-02 04:52:27.357006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.357323: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26f3320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-02 04:52:27.357345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2022-12-02 04:52:27.357640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.357834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-12-02 04:52:27.357870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-02 04:52:27.357905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-02 04:52:27.357917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-12-02 04:52:27.357927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-02 04:52:27.357938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-02 04:52:27.357949: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-12-02 04:52:27.357960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-12-02 04:52:27.357971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-02 04:52:27.358047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.358279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.358440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-12-02 04:52:27.358485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-02 04:52:27.907106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-12-02 04:52:27.907140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-12-02 04:52:27.907148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-12-02 04:52:27.907454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.907703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-02 04:52:27.907893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13971 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(model_params['anchor_box_scales']) * len(model_params['anchor_box_ratios'])\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "# create model for RPN\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "\n",
    "# define the classifier, built on the base layer and rpn\n",
    "classifier = nn.classifier(shared_layers, roi_input, num_rois, nb_classes=len(classes_count))\n",
    "# create model for object classification\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples 200002\n",
      "Num val samples 35925\n"
     ]
    }
   ],
   "source": [
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'train']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'val']\n",
    "\n",
    "random.shuffle(train_imgs)\n",
    "num_imgs = len(train_imgs)\n",
    "\n",
    "print(f'Num train samples {len(train_imgs)}')\n",
    "print(f'Num val samples {len(val_imgs)}')\n",
    "\n",
    "# define generators for ROI\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, config, model_params, nn.get_img_output_length, mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, config, model_params, nn.get_img_output_length, mode='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training based on previous trained model\n",
      "Loading weights from models/model_vgg16_open_images.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 04:52:30.287104: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 411041792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already train 274K batches\n"
     ]
    }
   ],
   "source": [
    "# Because I can only run the session several hours one time (then I need to connect again), \n",
    "# I need to save the model and load the model to continue training\n",
    "if not os.path.isfile(model_path):\n",
    "    # If this is the begin of the training, load the pre-traind base network such as vgg-16\n",
    "    try:\n",
    "        print('This is the first time of your training')\n",
    "        print('loading weights from {}'.format(base_net_weights))\n",
    "        model_rpn.load_weights(base_net_weights, by_name=True)\n",
    "        model_classifier.load_weights(base_net_weights, by_name=True)\n",
    "    except:\n",
    "        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "            https://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "    \n",
    "    # Create the record.csv file to record losses, acc and mAP\n",
    "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
    "else:\n",
    "    # If this is a continued training, load the trained model from before\n",
    "    print('Continue training based on previous trained model')\n",
    "    print('Loading weights from {}'.format(model_path))\n",
    "    model_rpn.load_weights(model_path, by_name=True)\n",
    "    model_classifier.load_weights(model_path, by_name=True)\n",
    "    \n",
    "    # Load the records\n",
    "    record_df = pd.read_csv(record_path)\n",
    "\n",
    "    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
    "    r_class_acc = record_df['class_acc']\n",
    "    r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
    "    r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
    "    r_loss_class_cls = record_df['loss_class_cls']\n",
    "    r_loss_class_regr = record_df['loss_class_regr']\n",
    "    r_curr_loss = record_df['curr_loss']\n",
    "    r_elapsed_time = record_df['elapsed_time']\n",
    "    r_mAP = record_df['mAP']\n",
    "\n",
    "    print('Already train %dK batches'% (len(record_df)))\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[model_losses.rpn_loss_cls(num_anchors), model_losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[model_losses.class_loss_cls, model_losses.class_loss_regr(len(classes_count)-1)], metrics={f'dense_class_{len(classes_count)}': 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "total_epochs = len(record_df)\n",
    "r_epochs = len(record_df)\n",
    "total_epochs += num_epochs\n",
    "\n",
    "epoch_length = 1000\n",
    "iter_num = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "\n",
    "if len(record_df)==0:\n",
    "    best_loss = np.Inf\n",
    "else:\n",
    "    best_loss = np.min(r_curr_loss.iloc[-10:])\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 275/294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 04:52:32.172855: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-12-02 04:52:32.344453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-02 04:52:33.512290: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-12-02 04:52:33.588180: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-12-02 04:52:34.376704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/3000 [..............................] - ETA: 20:12:49 - rpn_cls: 0.0401 - rpn_regr: 0.1550 - detector_cls: 3.4655 - detector_regr: 0.2223WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2f9c1d4f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2f4c6d3cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "   3/3000 [..............................] - ETA: 14:30:37 - rpn_cls: 0.1877 - rpn_regr: 0.1205 - detector_cls: 3.3399 - detector_regr: 0.2427WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2f9c1d4f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2f4c6d3cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1502/3000 [==============>...............] - ETA: 1:12:06 - rpn_cls: 2.0557 - rpn_regr: 0.3621 - detector_cls: 2.3626 - detector_regr: 0.3639Exception: 'a' cannot be empty unless no samples are taken\n",
      "2994/3000 [============================>.] - ETA: 12s - rpn_cls: 1.9973 - rpn_regr: 0.3534 - detector_cls: 2.3488 - detector_regr: 0.3683\n",
      "Average number of overlapping bounding boxes from RPN = 16.133333333333333 for 3000 previous iterations\n",
      "3000/3000 [==============================] - 6464s 2s/step - rpn_cls: 1.9955 - rpn_regr: 0.3534 - detector_cls: 2.3474 - detector_regr: 0.3683\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 16.12807717897538\n",
      "Classifier accuracy for bounding boxes from RPN: 0.5399166666666667\n",
      "Loss RPN classifier: 1.9954754365913037\n",
      "Loss RPN regression: 0.3534106462836886\n",
      "Loss Detector classifier: 2.347391406693806\n",
      "Loss Detector regression: 0.36829062302814175\n",
      "Elapsed time: 6463.8295176029205\n",
      "Epoch 276/294\n",
      "2983/3000 [============================>.] - ETA: 17s - rpn_cls: 2.0511 - rpn_regr: 0.3505 - detector_cls: 2.3581 - detector_regr: 0.3729\n",
      "Average number of overlapping bounding boxes from RPN = 16.441333333333333 for 3000 previous iterations\n",
      "3000/3000 [==============================] - 3155s 1s/step - rpn_cls: 2.0486 - rpn_regr: 0.3506 - detector_cls: 2.3560 - detector_regr: 0.3733\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 16.454001992693456\n",
      "Classifier accuracy for bounding boxes from RPN: 0.5399166666666667\n",
      "Loss RPN classifier: 2.04855075605782\n",
      "Loss RPN regression: 0.3505580909365478\n",
      "Loss Detector classifier: 2.3560255346521735\n",
      "Loss Detector regression: 0.3732577357647048\n",
      "Elapsed time: 3155.0202021598816\n",
      "Epoch 277/294\n",
      "1707/3000 [================>.............] - ETA: 20:38 - rpn_cls: 1.9296 - rpn_regr: 0.3388 - detector_cls: 2.3746 - detector_regr: 0.3690"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "start_time = time.time()\n",
    "vis = True\n",
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "\tprogbar = generic_utils.Progbar(epoch_length)\n",
    "\tprint('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n",
    "    \n",
    "\tr_epochs += 1\n",
    "\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and verbose:\n",
    "\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "\t\t\t\trpn_accuracy_rpn_monitor = []\n",
    "\t\t\t\tprint(f'\\nAverage number of overlapping bounding boxes from RPN = {mean_overlapping_bboxes} for {epoch_length} previous iterations')\n",
    "\t\t\t\tif mean_overlapping_bboxes == 0:\n",
    "\t\t\t\t\tprint('\\nRPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "\t\t\tX, Y, img_data = next(data_gen_train)\n",
    "\n",
    "\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "\t\t\tR = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], model_params, use_regr=True, max_boxes=300, overlap_thresh=0.7)\n",
    "\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "            # X2: bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "            # Y1: one hot code for bboxes from above => x_roi (X)\n",
    "            # Y2: corresponding labels and corresponding gt bboxes\n",
    "\t\t\tX2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, model_params, class_mapping)\n",
    "\n",
    "\t\t\tif X2 is None:\n",
    "\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n",
    "\t\t\t\trpn_accuracy_for_epoch.append(0)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "\t\t\tif len(neg_samples) > 0:\n",
    "\t\t\t\tneg_samples = neg_samples[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tneg_samples = []\n",
    "\n",
    "\t\t\tif len(pos_samples) > 0:\n",
    "\t\t\t\tpos_samples = pos_samples[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tpos_samples = []\n",
    "\t\t\t\n",
    "\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "\t\t\tif num_rois > 1:\n",
    "\t\t\t\tif len(pos_samples) < num_rois//2:\n",
    "\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, num_rois//2, replace=False).tolist()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n",
    "\t\t\telse:\n",
    "\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "\t\t\t\tselected_pos_samples = pos_samples.tolist()\n",
    "\t\t\t\tselected_neg_samples = neg_samples.tolist()\n",
    "\t\t\t\tif np.random.randint(0, 2):\n",
    "\t\t\t\t\tsel_samples = random.choice(neg_samples)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsel_samples = random.choice(pos_samples)\n",
    "\n",
    "\t\t\t# training_data: [X, X2[:, sel_samples, :]]\n",
    "            # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]\n",
    "            #  X                     => img_data resized image\n",
    "            #  X2[:, sel_samples, :] => num_rois (4 in here) bboxes which contains selected neg and pos\n",
    "            #  Y1[:, sel_samples, :] => one hot encode for num_rois bboxes which contains selected neg and pos\n",
    "            #  Y2[:, sel_samples, :] => labels and gt bboxes for num_rois bboxes which contains selected neg and pos\n",
    "\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n",
    "\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "\t\t\tlosses[iter_num, 2] = loss_class[1]\n",
    "\t\t\tlosses[iter_num, 3] = loss_class[2]\n",
    "\t\t\tlosses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "\t\t\tprogbar.update(iter_num+1, [('rpn_cls', losses[iter_num, 0]), ('rpn_regr', losses[iter_num, 1]),\n",
    "\t\t\t\t\t\t\t\t\t  ('detector_cls', losses[iter_num, 2]), ('detector_regr', losses[iter_num, 3])])\n",
    "\n",
    "\t\t\titer_num += 1\n",
    "\t\t\t\n",
    "\t\t\tif iter_num == epoch_length:\n",
    "\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n",
    "\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n",
    "\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n",
    "\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n",
    "\t\t\t\tclass_acc = np.mean(losses[:, 4])\n",
    "\n",
    "\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "\t\t\t\trpn_accuracy_for_epoch = []\n",
    "\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint(f'Mean number of bounding boxes from RPN overlapping ground truth boxes: {mean_overlapping_bboxes}')\n",
    "\t\t\t\t\tprint(f'Classifier accuracy for bounding boxes from RPN: {class_acc}')\n",
    "\t\t\t\t\tprint(f'Loss RPN classifier: {loss_rpn_cls}')\n",
    "\t\t\t\t\tprint(f'Loss RPN regression: {loss_rpn_regr}')\n",
    "\t\t\t\t\tprint(f'Loss Detector classifier: {loss_class_cls}')\n",
    "\t\t\t\t\tprint(f'Loss Detector regression: {loss_class_regr}')\n",
    "\t\t\t\t\tprint(f'Elapsed time: {time.time() - start_time}')\n",
    "\t\t\t\t\telapsed_time = (time.time()-start_time)/60\n",
    "\n",
    "\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "\t\t\t\titer_num = 0\n",
    "\t\t\t\tstart_time = time.time()\n",
    "\n",
    "\t\t\t\tif curr_loss < best_loss:\n",
    "\t\t\t\t\tif verbose:\n",
    "\t\t\t\t\t\tprint(f'Total loss decreased from {best_loss} to {curr_loss}, saving weights')\n",
    "\t\t\t\t\tbest_loss = curr_loss\n",
    "\t\t\t\t\tmodel_all.save_weights(model_path_regex.group(1) + model_path_regex.group(2))\n",
    "\n",
    "\t\t\t\tnew_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n",
    "                           'class_acc':round(class_acc, 3), \n",
    "                           'loss_rpn_cls':round(loss_rpn_cls, 3), \n",
    "                           'loss_rpn_regr':round(loss_rpn_regr, 3), \n",
    "                           'loss_class_cls':round(loss_class_cls, 3), \n",
    "                           'loss_class_regr':round(loss_class_regr, 3), \n",
    "                           'curr_loss':round(curr_loss, 3), \n",
    "                           'elapsed_time':round(elapsed_time, 3), \n",
    "                           'mAP': 0}\n",
    "\n",
    "\t\t\t\trecord_df = record_df.append(new_row, ignore_index=True)\n",
    "\t\t\t\trecord_df.to_csv(record_path, index=0)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f'Exception: {e}')\n",
    "\t\t\tcontinue\n",
    "\n",
    "print('Training complete, exiting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n",
    "plt.title('mean_overlapping_bboxes')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n",
    "plt.title('class_acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n",
    "plt.title('loss_rpn_cls')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n",
    "plt.title('loss_rpn_regr')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
    "plt.title('loss_class_cls')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n",
    "plt.title('loss_class_regr')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
    "plt.title('total_loss')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
    "# plt.title('total_loss')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n",
    "# plt.title('elapsed_time')\n",
    "# plt.show()\n",
    "\n",
    "# plt.title('loss')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'b')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'g')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
    "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'c')\n",
    "# # plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('object_detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "523c085937921c9bf1754be3fc06c3736dd9e909273ccc15755aad185ca77468"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
