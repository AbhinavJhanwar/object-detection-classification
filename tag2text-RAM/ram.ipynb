{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/xinyu1205/recognize-anything.git\n",
      "  Cloning https://github.com/xinyu1205/recognize-anything.git to /tmp/pip-req-build-pomaa_fx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/xinyu1205/recognize-anything.git /tmp/pip-req-build-pomaa_fx\n",
      "  Resolved https://github.com/xinyu1205/recognize-anything.git to commit 9752f2d60ea617b985598041961a0eea977edd60\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clip@ git+https://github.com/openai/CLIP.git (from ram==0.0.1)\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-gbyc5wx9/clip_ed115f5048d34e0aa2649b4de08b7854\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-gbyc5wx9/clip_ed115f5048d34e0aa2649b4de08b7854\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting timm==0.4.12 (from ram==0.0.1)\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.25.1 (from ram==0.0.1)\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting fairscale==0.4.4 (from ram==0.0.1)\n",
      "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycocoevalcap (from ram==0.0.1)\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (1.12.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (0.13.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (9.4.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (1.10.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from torch->ram==0.0.1) (4.4.0)\n",
      "Requirement already satisfied: ftfy in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1) (6.1.1)\n",
      "Collecting pycocotools>=2.0.2 (from pycocoevalcap->ram==0.0.1)\n",
      "  Downloading pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1->ram==0.0.1) (2022.8.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.5.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ftfy->clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1) (0.2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (2022.12.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.16.0)\n",
      "Building wheels for collected packages: ram, fairscale, clip\n",
      "  Building wheel for ram (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ram: filename=ram-0.0.1-py3-none-any.whl size=128517 sha256=e53c63134889b7c65907a794c39a73dc2b600818786bab978f3b39403b7279c9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a382nwxd/wheels/98/f7/d2/8a5ea2f7ccf0eb296d2e4ca84f1e4a89d2a262e26532e0ebf6\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292834 sha256=015b86011dae66890c95600fec6104fda11f836ee765af527595f5ffdde728d2\n",
      "  Stored in directory: /home/abhinav_jhanwar_valuelabs_com/.cache/pip/wheels/b3/c3/6e/3fca67aaef3657c2266e9ee439b54f534f05967cd8774cc65b\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369501 sha256=e26265cf109ea3821ea771435a37250360db0ea90c61eee2b37a548f4e1f93ba\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a382nwxd/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "Successfully built ram fairscale clip\n",
      "Installing collected packages: fairscale, transformers, timm, pycocotools, clip, pycocoevalcap, ram\n",
      "  Attempting uninstall: fairscale\n",
      "    Found existing installation: fairscale 0.4.13\n",
      "    Uninstalling fairscale-0.4.13:\n",
      "      Successfully uninstalled fairscale-0.4.13\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0.dev0\n",
      "    Uninstalling transformers-4.31.0.dev0:\n",
      "      Successfully uninstalled transformers-4.31.0.dev0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keyphrase-extraction 1.0 requires huggingface-hub==0.11.1, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "keyphrase-extraction 1.0 requires Jinja2==3.1.2, but you have jinja2 2.11.3 which is incompatible.\n",
      "keyphrase-extraction 1.0 requires MarkupSafe==2.1.2, but you have markupsafe 1.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed clip-1.0 fairscale-0.4.4 pycocoevalcap-1.2 pycocotools-2.0.7 ram-0.0.1 timm-0.4.12 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "# Install recognize-anything as a package\n",
    "! pip install git+https://github.com/xinyu1205/recognize-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ram_plus_swin_large_14m.pth', <http.client.HTTPMessage at 0x7ff39f8fea90>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download RAM++ checkpoint\n",
    "# https://huggingface.co/xinyu1205/recognize-anything-plus-model/blob/main/ram_plus_swin_large_14m.pth\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth\", \"ram_plus_swin_large_14m.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * The Recognize Anything Plus Model (RAM++)\n",
    " * Written by Xinyu Huang\n",
    "'''\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import inference_ram as inference\n",
    "from ram import get_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "ram_plus_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from ram_plus_swin_large_14m.pth\n",
      "vit: swin_l\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "                image_size=384,\n",
    "                pretrained='ram_plus_swin_large_14m.pth',\n",
    "                image='image classification images/lifestyle/1.jpg')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = get_transform(image_size=args.image_size)\n",
    "\n",
    "#######load model\n",
    "model = ram_plus(pretrained=args.pretrained,\n",
    "                image_size=args.image_size,\n",
    "                vit='swin_l')\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[39m=\u001b[39m Namespace(\n\u001b[1;32m      5\u001b[0m image_size\u001b[39m=\u001b[39m\u001b[39m384\u001b[39m,\n\u001b[1;32m      6\u001b[0m pretrained\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mram_plus_swin_large_14m.pth\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m image\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage classification images/Product/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39mi)\n\u001b[1;32m      9\u001b[0m image \u001b[39m=\u001b[39m transform(Image\u001b[39m.\u001b[39mopen(args\u001b[39m.\u001b[39mimage))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m res \u001b[39m=\u001b[39m inference(image, model)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mImage Tags: \u001b[39m\u001b[39m\"\u001b[39m, res[\u001b[39m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m图像标签: \u001b[39m\u001b[39m\"\u001b[39m, res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/inference.py:36\u001b[0m, in \u001b[0;36minference_ram\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference_ram\u001b[39m(image, model):\n\u001b[1;32m     35\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m         tags, tags_chinese \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate_tag(image)\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m tags[\u001b[39m0\u001b[39m],tags_chinese[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/ram_plus.py:291\u001b[0m, in \u001b[0;36mRAM_plus.generate_tag\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    288\u001b[0m logits_per_image \u001b[39m=\u001b[39m logits_per_image\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,des_per_class)\n\u001b[1;32m    290\u001b[0m weight_normalized \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits_per_image, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m label_embed_reweight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty(bs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_class, \u001b[39m512\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    292\u001b[0m weight_normalized \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits_per_image, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    293\u001b[0m label_embed_reweight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(bs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_class, \u001b[39m512\u001b[39m)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Namespace(\n",
    "image_size=384,\n",
    "pretrained='ram_plus_swin_large_14m.pth',\n",
    "image='image classification images/Product/1.jpg')\n",
    "\n",
    "image = transform(Image.open(args.image)).unsqueeze(0).to(device)\n",
    "\n",
    "res = inference(image, model)\n",
    "print(\"Image Tags: \", res[0])\n",
    "print(\"图像标签: \", res[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
