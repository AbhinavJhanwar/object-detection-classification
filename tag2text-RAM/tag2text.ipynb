{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/xinyu1205/recognize-anything.git\n",
      "  Cloning https://github.com/xinyu1205/recognize-anything.git to /tmp/pip-req-build-pomaa_fx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/xinyu1205/recognize-anything.git /tmp/pip-req-build-pomaa_fx\n",
      "  Resolved https://github.com/xinyu1205/recognize-anything.git to commit 9752f2d60ea617b985598041961a0eea977edd60\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clip@ git+https://github.com/openai/CLIP.git (from ram==0.0.1)\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-gbyc5wx9/clip_ed115f5048d34e0aa2649b4de08b7854\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-gbyc5wx9/clip_ed115f5048d34e0aa2649b4de08b7854\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting timm==0.4.12 (from ram==0.0.1)\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.25.1 (from ram==0.0.1)\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting fairscale==0.4.4 (from ram==0.0.1)\n",
      "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycocoevalcap (from ram==0.0.1)\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (1.12.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (0.13.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (9.4.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ram==0.0.1) (1.10.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from transformers==4.25.1->ram==0.0.1) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from torch->ram==0.0.1) (4.4.0)\n",
      "Requirement already satisfied: ftfy in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1) (6.1.1)\n",
      "Collecting pycocotools>=2.0.2 (from pycocoevalcap->ram==0.0.1)\n",
      "  Downloading pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1->ram==0.0.1) (2022.8.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.5.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from ftfy->clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1) (0.2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from requests->transformers==4.25.1->ram==0.0.1) (2022.12.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/st_env1/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.16.0)\n",
      "Building wheels for collected packages: ram, fairscale, clip\n",
      "  Building wheel for ram (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ram: filename=ram-0.0.1-py3-none-any.whl size=128517 sha256=e53c63134889b7c65907a794c39a73dc2b600818786bab978f3b39403b7279c9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a382nwxd/wheels/98/f7/d2/8a5ea2f7ccf0eb296d2e4ca84f1e4a89d2a262e26532e0ebf6\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292834 sha256=015b86011dae66890c95600fec6104fda11f836ee765af527595f5ffdde728d2\n",
      "  Stored in directory: /home/abhinav_jhanwar_valuelabs_com/.cache/pip/wheels/b3/c3/6e/3fca67aaef3657c2266e9ee439b54f534f05967cd8774cc65b\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369501 sha256=e26265cf109ea3821ea771435a37250360db0ea90c61eee2b37a548f4e1f93ba\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a382nwxd/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "Successfully built ram fairscale clip\n",
      "Installing collected packages: fairscale, transformers, timm, pycocotools, clip, pycocoevalcap, ram\n",
      "  Attempting uninstall: fairscale\n",
      "    Found existing installation: fairscale 0.4.13\n",
      "    Uninstalling fairscale-0.4.13:\n",
      "      Successfully uninstalled fairscale-0.4.13\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0.dev0\n",
      "    Uninstalling transformers-4.31.0.dev0:\n",
      "      Successfully uninstalled transformers-4.31.0.dev0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keyphrase-extraction 1.0 requires huggingface-hub==0.11.1, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "keyphrase-extraction 1.0 requires Jinja2==3.1.2, but you have jinja2 2.11.3 which is incompatible.\n",
      "keyphrase-extraction 1.0 requires MarkupSafe==2.1.2, but you have markupsafe 1.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed clip-1.0 fairscale-0.4.4 pycocoevalcap-1.2 pycocotools-2.0.7 ram-0.0.1 timm-0.4.12 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "# Install recognize-anything as a package\n",
    "! pip install git+https://github.com/xinyu1205/recognize-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tag2text_swin_14m.pth', <http.client.HTTPMessage at 0x7f526024eee0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download tag2text checkpoint\n",
    "# https://huggingface.co/spaces/xinyu1205/recognize-anything/blob/main/tag2text_swin_14m.pth\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://huggingface.co/spaces/xinyu1205/recognize-anything/resolve/main/tag2text_swin_14m.pth\", \"tag2text_swin_14m.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * The Tag2Text Model\n",
    " * Written by Xinyu Huang\n",
    "'''\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from ram.models import tag2text\n",
    "from ram import inference_tag2text as inference\n",
    "from ram import get_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/encoder/layer/0/crossattention/self/query is tied\n",
      "/encoder/layer/0/crossattention/self/key is tied\n",
      "/encoder/layer/0/crossattention/self/value is tied\n",
      "/encoder/layer/0/crossattention/output/dense is tied\n",
      "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/0/intermediate/dense is tied\n",
      "/encoder/layer/0/output/dense is tied\n",
      "/encoder/layer/0/output/LayerNorm is tied\n",
      "/encoder/layer/1/crossattention/self/query is tied\n",
      "/encoder/layer/1/crossattention/self/key is tied\n",
      "/encoder/layer/1/crossattention/self/value is tied\n",
      "/encoder/layer/1/crossattention/output/dense is tied\n",
      "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/1/intermediate/dense is tied\n",
      "/encoder/layer/1/output/dense is tied\n",
      "/encoder/layer/1/output/LayerNorm is tied\n",
      "--------------\n",
      "tag2text_swin_14m.pth\n",
      "--------------\n",
      "load checkpoint from tag2text_swin_14m.pth\n",
      "vit: swin_b\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "description='Tag2Text inferece for tagging and captioning',\n",
    "image_size=384,\n",
    "pretrained='tag2text_swin_14m.pth',\n",
    "image='image classification images/lifestyle/1.jpg',\n",
    "thre=0.68,\n",
    "specified_tags='sugar')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = get_transform(image_size=args.image_size)\n",
    "\n",
    "# delete some tags that may disturb captioning\n",
    "# 127: \"quarter\"; 2961: \"back\", 3351: \"two\"; 3265: \"three\"; 3338: \"four\"; 3355: \"five\"; 3359: \"one\"\n",
    "delete_tag_index = [127,2961, 3351, 3265, 3338, 3355, 3359]\n",
    "\n",
    "#######load model\n",
    "model = tag2text(pretrained=args.pretrained,\n",
    "                image_size=args.image_size,\n",
    "                vit='swin_b',\n",
    "                delete_tag_index=delete_tag_index)\n",
    "model.threshold = args.thre  # threshold for tagging\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Identified Tags:  drink | cup | straw | flavor | product | bottle | beverage | fruit | juice\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a group of bottles with straws and cups with fruit in each flavor\n",
      "17.jpg\n",
      "Model Identified Tags:  snack | bar | box\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of energy bars with chocolate and peanut butter flavored bars\n",
      "40.jpg\n",
      "Model Identified Tags:  coffee | cup | flavor | box\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a cup of coffee on a white background\n",
      "4.jpg\n",
      "Model Identified Tags:  drink | brand | vodka | alcohol | bottle | beverage\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of beverage is the most popular drink\n",
      "28.jpg\n",
      "Model Identified Tags:  pasta | noodle | food | cheese | box | broccoli\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of noodles with broccoli and cheese\n",
      "68.jpg\n",
      "Model Identified Tags:  perfume | bottle | man\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of the perfume that was launched\n",
      "76.jpg\n",
      "Model Identified Tags:  bottle | whiskey | beverage\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of beverage, made\n",
      "14.jpg\n",
      "Model Identified Tags:  product | box | bottle | pack\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of product on a white background\n",
      "46.jpg\n",
      "Model Identified Tags:  product | bottle\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of shampoo and conditioner on a white background\n",
      "66.jpg\n",
      "Model Identified Tags:  brand | pack | new\n",
      "User Specified Tags:  None\n",
      "Image Caption:  brand on packaging of the package design gallery\n",
      "48.jpg\n",
      "Model Identified Tags:  image | product | bottle | skin | white\n",
      "User Specified Tags:  None\n",
      "Image Caption:  an image of a product in a white bottle\n",
      "59.jpg\n",
      "Model Identified Tags:  toothpaste | toothbrush | box | pack | blue\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a blue box of toothpaste on a white background\n",
      "62.jpg\n",
      "Model Identified Tags:  toothpaste | product | bottle | tooth | blue\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a blue bottle of toothpaste on a white background\n",
      "31.jpg\n",
      "Model Identified Tags:  coffee | box | pack\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of coffee, each with a different flavored beverage type\n",
      "83.jpg\n",
      "Model Identified Tags:  shelf | bookshelf | shelving | wood | bookcase | wooden\n",
      "User Specified Tags:  None\n",
      "Image Caption:  the bookshelf is made of wooden shelves\n",
      "9.jpg\n",
      "Model Identified Tags:  image | food | bottle\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of food on a white background\n",
      "37.jpg\n",
      "Model Identified Tags:  label | sticker | box | logo\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box with various types of stickers on it\n",
      "36.jpg\n",
      "Model Identified Tags:  coffee | cup | box | pack\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of coffee, each with a different flavored beverage type\n",
      "43.jpg\n",
      "Model Identified Tags:  hair | product | bottle\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of shampoo and conditioner on a white background\n",
      "29.jpg\n",
      "Model Identified Tags:  business | pasta | brand | flavor | food | product | cheese\n",
      "User Specified Tags:  None\n",
      "Image Caption:  brand is a brand of food made from canned food and cheese\n",
      "70.jpg\n",
      "Model Identified Tags:  bottle | whiskey\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of whisky is pictured\n",
      "81.jpg\n",
      "Model Identified Tags:  shelf | bookshelf | shelving | wood | bookcase | wooden\n",
      "User Specified Tags:  None\n",
      "Image Caption:  the bookshelf is made of wooden shelves\n",
      "71.jpg\n",
      "Model Identified Tags:  drink | bottle | whiskey\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of single malt whisky\n",
      "53.jpg\n",
      "Model Identified Tags:  book\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a book about the power of positive thinking\n",
      "13.jpg\n",
      "Model Identified Tags:  snack | chip | flavor | food | box\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of chips with flavors on a white background\n",
      "20.jpg\n",
      "Model Identified Tags:  snack | brand | chip | flavor | food | product | box\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of brand, a flavor of chips\n",
      "58.jpg\n",
      "Model Identified Tags:  brand | product | bottle\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of cleaner and a bottle of cleaner\n",
      "26.jpg\n",
      "Model Identified Tags:  brand | food | cheese | box\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a box of brand on a white background\n",
      "7.jpg\n",
      "Model Identified Tags:  drink | product | bottle\n",
      "User Specified Tags:  None\n",
      "Image Caption:  bottles of beverage type on a white background\n",
      "75.jpg\n",
      "Model Identified Tags:  bottle | whiskey\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of whiskey on a white background\n",
      "1.jpg\n",
      "Model Identified Tags:  drink | bottle | beverage | glass\n",
      "User Specified Tags:  None\n",
      "Image Caption:  a bottle of beverage on a white background\n",
      "18.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[39m=\u001b[39m Namespace(\n\u001b[1;32m      5\u001b[0m description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTag2Text inferece for tagging and captioning\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m image_size\u001b[39m=\u001b[39m\u001b[39m384\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# specify custome tags separated by comma\u001b[39;00m\n\u001b[1;32m     11\u001b[0m specified_tags\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m image \u001b[39m=\u001b[39m transform(Image\u001b[39m.\u001b[39mopen(args\u001b[39m.\u001b[39mimage))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m res \u001b[39m=\u001b[39m inference(image, model, args\u001b[39m.\u001b[39;49mspecified_tags)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Identified Tags: \u001b[39m\u001b[39m\"\u001b[39m, res[\u001b[39m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUser Specified Tags: \u001b[39m\u001b[39m\"\u001b[39m, res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/inference.py:11\u001b[0m, in \u001b[0;36minference_tag2text\u001b[0;34m(image, model, input_tag)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference_tag2text\u001b[39m(image, model, input_tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         caption, tag_predict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(image,\n\u001b[1;32m     12\u001b[0m                                               tag_input\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     13\u001b[0m                                               max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m                                               return_tag_predict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m input_tag \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m input_tag \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m input_tag \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m         \u001b[39mreturn\u001b[39;00m tag_predict[\u001b[39m0\u001b[39m], \u001b[39mNone\u001b[39;00m, caption[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/tag2text.py:349\u001b[0m, in \u001b[0;36mTag2Text.generate\u001b[0;34m(self, image, sample, num_beams, max_length, min_length, top_p, repetition_penalty, tag_input, return_tag_predict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[39m# beam search (default)\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     model_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    346\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mencoder_hidden_states\u001b[39m\u001b[39m\"\u001b[39m: output_tagembedding\u001b[39m.\u001b[39mlast_hidden_state,\n\u001b[1;32m    347\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mencoder_attention_mask\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     }\n\u001b[0;32m--> 349\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    350\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    351\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    352\u001b[0m         min_length\u001b[39m=\u001b[39;49mmin_length,\n\u001b[1;32m    353\u001b[0m         num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    354\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49msep_token_id,\n\u001b[1;32m    355\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m    356\u001b[0m         repetition_penalty\u001b[39m=\u001b[39;49mrepetition_penalty,\n\u001b[1;32m    357\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    359\u001b[0m captions \u001b[39m=\u001b[39m []\n\u001b[1;32m    360\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/transformers/generation/utils.py:1608\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1602\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1603\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_beams,\n\u001b[1;32m   1604\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1605\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1606\u001b[0m     )\n\u001b[1;32m   1607\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1608\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1609\u001b[0m         input_ids,\n\u001b[1;32m   1610\u001b[0m         beam_scorer,\n\u001b[1;32m   1611\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1612\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1613\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1614\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1615\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1616\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1617\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1618\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1622\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1624\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1625\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         renormalize_logits\u001b[39m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1630\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/transformers/generation/utils.py:2799\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2795\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2799\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2800\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2801\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2802\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2803\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2804\u001b[0m )\n\u001b[1;32m   2806\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2807\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:960\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction, mode)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m    961\u001b[0m     input_ids,\n\u001b[1;32m    962\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    963\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    964\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    965\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    966\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    967\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    968\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    969\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    970\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    971\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    972\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    973\u001b[0m     is_decoder\u001b[39m=\u001b[39;49mis_decoder,\n\u001b[1;32m    974\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    975\u001b[0m )\n\u001b[1;32m    977\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    978\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:856\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder, mode)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     embedding_output \u001b[39m=\u001b[39m encoder_embeds\n\u001b[0;32m--> 856\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    857\u001b[0m     embedding_output,\n\u001b[1;32m    858\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    859\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    860\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    861\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    862\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    863\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    864\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    865\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    866\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    867\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    868\u001b[0m )\n\u001b[1;32m    869\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    870\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:520\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, mode)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    511\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    521\u001b[0m         hidden_states,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         layer_head_mask,\n\u001b[1;32m    524\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    526\u001b[0m         past_key_value,\n\u001b[1;32m    527\u001b[0m         output_attentions,\n\u001b[1;32m    528\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:436\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m mode\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmultimodal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    434\u001b[0m     \u001b[39massert\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mencoder_hidden_states must be given for cross-attention layers\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 436\u001b[0m     cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossattention(\n\u001b[1;32m    437\u001b[0m         attention_output,\n\u001b[1;32m    438\u001b[0m         attention_mask,\n\u001b[1;32m    439\u001b[0m         head_mask,\n\u001b[1;32m    440\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    441\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    442\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m     attention_output \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# add cross attentions if we output attention weights                               \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:333\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ):\n\u001b[0;32m--> 333\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    334\u001b[0m         hidden_states,\n\u001b[1;32m    335\u001b[0m         attention_mask,\n\u001b[1;32m    336\u001b[0m         head_mask,\n\u001b[1;32m    337\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    338\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    339\u001b[0m         past_key_value,\n\u001b[1;32m    340\u001b[0m         output_attentions,\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    343\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/ram/models/bert.py:212\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m is_cross_attention:\n\u001b[1;32m    210\u001b[0m     \u001b[39m# print(self.key.weight.shape)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(encoder_hidden_states))\n\u001b[0;32m--> 212\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(encoder_hidden_states))\n\u001b[1;32m    213\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    214\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/st_env1/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir('image classification images/Product'):\n",
    "    print(i)\n",
    "    args = Namespace(\n",
    "    description='Tag2Text inferece for tagging and captioning',\n",
    "    image_size=384,\n",
    "    pretrained='tag2text_swin_14m.pth',\n",
    "    image='image classification images/Product/%s'%i,\n",
    "    thre=0.68,\n",
    "    # specify custome tags separated by comma\n",
    "    specified_tags='')\n",
    "\n",
    "    image = transform(Image.open(args.image)).unsqueeze(0).to(device)\n",
    "\n",
    "    res = inference(image, model, args.specified_tags)\n",
    "    print(\"Model Identified Tags: \", res[0])\n",
    "    print(\"User Specified Tags: \", res[1])\n",
    "    print(\"Image Caption: \", res[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
