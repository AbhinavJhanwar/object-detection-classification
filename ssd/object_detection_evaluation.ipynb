{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from eval_utils.average_precision_evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASCAL VOC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few configuration parameters.\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "n_classes = 20\n",
    "model_mode = 'inference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load trained SSD\n",
    "You can find the download links to all the trained model weights in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the model and load trained weights into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0609 15:41:42.513583  7272 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0609 15:41:42.526578  7272 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\Documents\\git\\object-detection-classification\\ssd\\keras_loss_function\\keras_ssd_loss.py:95: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0609 15:41:42.549485  7272 deprecation.py:323] From C:\\Users\\abhinav.jhanwar\\Documents\\git\\object-detection-classification\\ssd\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, 3),\n",
    "                n_classes=20,\n",
    "                mode='inference',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 100, 300],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=0.5,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "weights_path = 'pretrained_weights/VGG_VOC0712_SSD_300x300_iter_120000.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "model_path = 'model.h5'\n",
    "\n",
    "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                               'L2Normalization': L2Normalization,\n",
    "                                               'DecodeDetections': DecodeDetections,\n",
    "                                               'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a data generator for the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'test.txt': 100%|████████████████████████████████████████████| 4952/4952 [00:26<00:00, 190.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = DataGenerator()\n",
    "\n",
    "# TODO: Set the paths to the dataset here.\n",
    "images_dir = 'pascal voc/test/JPEGImages/'\n",
    "annotations_dir = 'pascal voc/test/Annotations/'\n",
    "image_set_filename = 'pascal voc/test/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset.parse_xml(images_dirs=[images_dir],\n",
    "                  image_set_filenames=[image_set_filename],\n",
    "                  annotations_dirs=[annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the evaluation\n",
    "The evaluator roughly performs the following steps: It runs predictions over the entire given dataset, then it matches these predictions to the ground truth boxes, then it computes the precision-recall curves for each class, then it samples 11 equidistant points from these precision-recall curves to compute the average precision for each class, and finally it computes the mean average precision over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the evaluation dataset: 4952\n",
      "\n",
      "Producing predictions batch-wise: 100%|██████████████████████████████████████████████| 619/619 [34:24<00:00,  3.33s/it]\n",
      "Matching predictions to ground truth, class 1/20.: 100%|██████████████████████████| 240/240 [00:00<00:00, 10022.83it/s]\n",
      "Matching predictions to ground truth, class 2/20.: 100%|██████████████████████████| 295/295 [00:00<00:00, 10564.46it/s]\n",
      "Matching predictions to ground truth, class 3/20.: 100%|███████████████████████████| 358/358 [00:00<00:00, 8965.35it/s]\n",
      "Matching predictions to ground truth, class 4/20.: 100%|██████████████████████████| 229/229 [00:00<00:00, 10933.73it/s]\n",
      "Matching predictions to ground truth, class 5/20.: 100%|██████████████████████████| 203/203 [00:00<00:00, 11289.06it/s]\n",
      "Matching predictions to ground truth, class 6/20.: 100%|██████████████████████████| 203/203 [00:00<00:00, 11288.91it/s]\n",
      "Matching predictions to ground truth, class 7/20.: 100%|████████████████████████| 1135/1135 [00:00<00:00, 12509.16it/s]\n",
      "Matching predictions to ground truth, class 8/20.: 100%|██████████████████████████| 340/340 [00:00<00:00, 12175.98it/s]\n",
      "Matching predictions to ground truth, class 9/20.: 100%|██████████████████████████| 687/687 [00:00<00:00, 11286.58it/s]\n",
      "Matching predictions to ground truth, class 10/20.: 100%|█████████████████████████| 218/218 [00:00<00:00, 10932.59it/s]\n",
      "Matching predictions to ground truth, class 11/20.: 100%|█████████████████████████| 264/264 [00:00<00:00, 13924.58it/s]\n",
      "Matching predictions to ground truth, class 12/20.: 100%|█████████████████████████| 511/511 [00:00<00:00, 13493.98it/s]\n",
      "Matching predictions to ground truth, class 13/20.: 100%|█████████████████████████| 329/329 [00:00<00:00, 11767.29it/s]\n",
      "Matching predictions to ground truth, class 14/20.: 100%|█████████████████████████| 289/289 [00:00<00:00, 11749.89it/s]\n",
      "Matching predictions to ground truth, class 15/20.: 100%|███████████████████████| 3655/3655 [00:00<00:00, 11708.53it/s]\n",
      "Matching predictions to ground truth, class 16/20.: 100%|█████████████████████████| 269/269 [00:00<00:00, 12241.43it/s]\n",
      "Matching predictions to ground truth, class 17/20.: 100%|█████████████████████████| 208/208 [00:00<00:00, 12245.28it/s]\n",
      "Matching predictions to ground truth, class 18/20.: 100%|█████████████████████████| 370/370 [00:00<00:00, 14821.57it/s]\n",
      "Matching predictions to ground truth, class 19/20.: 100%|█████████████████████████| 278/278 [00:00<00:00, 10335.19it/s]\n",
      "Matching predictions to ground truth, class 20/20.: 100%|█████████████████████████| 307/307 [00:00<00:00, 11145.79it/s]\n",
      "Computing precisions and recalls, class 1/20\n",
      "Computing precisions and recalls, class 2/20\n",
      "Computing precisions and recalls, class 3/20\n",
      "Computing precisions and recalls, class 4/20\n",
      "Computing precisions and recalls, class 5/20\n",
      "Computing precisions and recalls, class 6/20\n",
      "Computing precisions and recalls, class 7/20\n",
      "Computing precisions and recalls, class 8/20\n",
      "Computing precisions and recalls, class 9/20\n",
      "Computing precisions and recalls, class 10/20\n",
      "Computing precisions and recalls, class 11/20\n",
      "Computing precisions and recalls, class 12/20\n",
      "Computing precisions and recalls, class 13/20\n",
      "Computing precisions and recalls, class 14/20\n",
      "Computing precisions and recalls, class 15/20\n",
      "Computing precisions and recalls, class 16/20\n",
      "Computing precisions and recalls, class 17/20\n",
      "Computing precisions and recalls, class 18/20\n",
      "Computing precisions and recalls, class 19/20\n",
      "Computing precisions and recalls, class 20/20\n",
      "Computing average precision, class 1/20\n",
      "Computing average precision, class 2/20\n",
      "Computing average precision, class 3/20\n",
      "Computing average precision, class 4/20\n",
      "Computing average precision, class 5/20\n",
      "Computing average precision, class 6/20\n",
      "Computing average precision, class 7/20\n",
      "Computing average precision, class 8/20\n",
      "Computing average precision, class 9/20\n",
      "Computing average precision, class 10/20\n",
      "Computing average precision, class 11/20\n",
      "Computing average precision, class 12/20\n",
      "Computing average precision, class 13/20\n",
      "Computing average precision, class 14/20\n",
      "Computing average precision, class 15/20\n",
      "Computing average precision, class 16/20\n",
      "Computing average precision, class 17/20\n",
      "Computing average precision, class 18/20\n",
      "Computing average precision, class 19/20\n",
      "Computing average precision, class 20/20\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(model=model,\n",
    "                      n_classes=n_classes,\n",
    "                      data_generator=dataset,\n",
    "                      model_mode=model_mode)\n",
    "\n",
    "results = evaluator(img_height=img_height,\n",
    "                    img_width=img_width,\n",
    "                    batch_size=8,\n",
    "                    data_generator_mode='resize',\n",
    "                    round_confidences=False,\n",
    "                    matching_iou_threshold=0.5,\n",
    "                    border_pixels='include',\n",
    "                    sorting_algorithm='quicksort',\n",
    "                    average_precision_mode='sample',\n",
    "                    num_recall_points=11,\n",
    "                    ignore_neutral_boxes=True,\n",
    "                    return_precisions=True,\n",
    "                    return_recalls=True,\n",
    "                    return_average_precisions=True,\n",
    "                    verbose=True)\n",
    "\n",
    "mean_average_precision, average_precisions, precisions, recalls = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane     AP    0.71\n",
      "bicycle       AP    0.721\n",
      "bird          AP    0.62\n",
      "boat          AP    0.595\n",
      "bottle        AP    0.339\n",
      "bus           AP    0.8\n",
      "car           AP    0.8\n",
      "cat           AP    0.809\n",
      "chair         AP    0.485\n",
      "cow           AP    0.698\n",
      "diningtable   AP    0.681\n",
      "dog           AP    0.798\n",
      "horse         AP    0.809\n",
      "motorbike     AP    0.72\n",
      "person        AP    0.622\n",
      "pottedplant   AP    0.341\n",
      "sheep         AP    0.625\n",
      "sofa          AP    0.755\n",
      "train         AP    0.809\n",
      "tvmonitor     AP    0.694\n",
      "\n",
      "              mAP   0.672\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(average_precisions)):\n",
    "    print(\"{:<14}{:<6}{}\".format(classes[i], 'AP', round(average_precisions[i], 3)))\n",
    "print()\n",
    "print(\"{:<14}{:<6}{}\".format('','mAP', round(mean_average_precision, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
