{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from eval_utils.average_precision_evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASCAL VOC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few configuration parameters.\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "n_classes = 20\n",
    "model_mode = 'inference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load trained SSD\n",
    "You can find the download links to all the trained model weights in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the model and load trained weights into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0608 23:31:04.717632 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0608 23:31:04.722708 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0608 23:31:04.794028 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0608 23:31:04.799753 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0608 23:31:04.831801 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0608 23:31:04.893237 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0608 23:31:06.341963 24372 deprecation.py:323] From C:\\Users\\abhinav.jhanwar\\Documents\\git\\object-detection-classification\\ssd\\keras_layers\\keras_layer_DecodeDetections.py:174: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0608 23:31:06.384240 24372 deprecation.py:323] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0608 23:31:06.983254 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0608 23:31:06.983254 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0608 23:31:06.983254 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0608 23:31:08.094234 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0608 23:31:08.094234 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0608 23:31:08.395988 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "W0608 23:31:09.458111 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0608 23:31:09.483342 24372 module_wrapper.py:139] From C:\\Users\\abhinav.jhanwar\\Documents\\git\\object-detection-classification\\ssd\\keras_loss_function\\keras_ssd_loss.py:95: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0608 23:31:09.535229 24372 deprecation.py:323] From C:\\Users\\abhinav.jhanwar\\Documents\\git\\object-detection-classification\\ssd\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, 3),\n",
    "                n_classes=20,\n",
    "                mode='inference',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 100, 300],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=0.5,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "weights_path = 'pretrained_weights/ssd300_pascal_07+12_102k_steps.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "model_path = 'model.h5'\n",
    "\n",
    "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                               'L2Normalization': L2Normalization,\n",
    "                                               'DecodeDetections': DecodeDetections,\n",
    "                                               'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a data generator for the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'test.txt': 100%|████████████████████████████████████████████| 4952/4952 [00:35<00:00, 141.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = DataGenerator()\n",
    "\n",
    "# TODO: Set the paths to the dataset here.\n",
    "images_dir = 'pascal voc/test/JPEGImages/'\n",
    "annotations_dir = 'pascal voc/test/Annotations/'\n",
    "image_set_filename = 'pascal voc/test/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset.parse_xml(images_dirs=[images_dir],\n",
    "                  image_set_filenames=[image_set_filename],\n",
    "                  annotations_dirs=[annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the evaluation\n",
    "The evaluator roughly performs the following steps: It runs predictions over the entire given dataset, then it matches these predictions to the ground truth boxes, then it computes the precision-recall curves for each class, then it samples 11 equidistant points from these precision-recall curves to compute the average precision for each class, and finally it computes the mean average precision over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the evaluation dataset: 4952\n",
      "\n",
      "Producing predictions batch-wise: 100%|██████████████████████████████████████████████| 619/619 [44:38<00:00,  4.33s/it]\n",
      "Matching predictions to ground truth, class 1/20.: 100%|███████████████████████████| 221/221 [00:00<00:00, 3262.98it/s]\n",
      "Matching predictions to ground truth, class 2/20.: 100%|███████████████████████████| 289/289 [00:00<00:00, 7900.06it/s]\n",
      "Matching predictions to ground truth, class 3/20.: 100%|███████████████████████████| 357/357 [00:00<00:00, 5842.42it/s]\n",
      "Matching predictions to ground truth, class 4/20.: 100%|███████████████████████████| 213/213 [00:00<00:00, 5007.94it/s]\n",
      "Matching predictions to ground truth, class 5/20.: 100%|███████████████████████████| 175/175 [00:00<00:00, 5310.09it/s]\n",
      "Matching predictions to ground truth, class 6/20.: 100%|███████████████████████████| 196/196 [00:00<00:00, 5317.38it/s]\n",
      "Matching predictions to ground truth, class 7/20.: 100%|█████████████████████████| 1069/1069 [00:00<00:00, 4990.13it/s]\n",
      "Matching predictions to ground truth, class 8/20.: 100%|███████████████████████████| 321/321 [00:00<00:00, 6626.33it/s]\n",
      "Matching predictions to ground truth, class 9/20.: 100%|███████████████████████████| 656/656 [00:00<00:00, 5787.96it/s]\n",
      "Matching predictions to ground truth, class 10/20.: 100%|██████████████████████████| 223/223 [00:00<00:00, 5482.91it/s]\n",
      "Matching predictions to ground truth, class 11/20.: 100%|██████████████████████████| 237/237 [00:00<00:00, 7674.11it/s]\n",
      "Matching predictions to ground truth, class 12/20.: 100%|██████████████████████████| 489/489 [00:00<00:00, 4347.06it/s]\n",
      "Matching predictions to ground truth, class 13/20.: 100%|██████████████████████████| 325/325 [00:00<00:00, 4914.55it/s]\n",
      "Matching predictions to ground truth, class 14/20.: 100%|██████████████████████████| 277/277 [00:00<00:00, 6820.33it/s]\n",
      "Matching predictions to ground truth, class 15/20.: 100%|████████████████████████| 3596/3596 [00:00<00:00, 5442.73it/s]\n",
      "Matching predictions to ground truth, class 16/20.: 100%|██████████████████████████| 264/264 [00:00<00:00, 6805.17it/s]\n",
      "Matching predictions to ground truth, class 17/20.: 100%|██████████████████████████| 187/187 [00:00<00:00, 6391.26it/s]\n",
      "Matching predictions to ground truth, class 18/20.: 100%|██████████████████████████| 348/348 [00:00<00:00, 7625.17it/s]\n",
      "Matching predictions to ground truth, class 19/20.: 100%|██████████████████████████| 270/270 [00:00<00:00, 7337.50it/s]\n",
      "Matching predictions to ground truth, class 20/20.: 100%|██████████████████████████| 272/272 [00:00<00:00, 6864.53it/s]\n",
      "Computing precisions and recalls, class 1/20\n",
      "Computing precisions and recalls, class 2/20\n",
      "Computing precisions and recalls, class 3/20\n",
      "Computing precisions and recalls, class 4/20\n",
      "Computing precisions and recalls, class 5/20\n",
      "Computing precisions and recalls, class 6/20\n",
      "Computing precisions and recalls, class 7/20\n",
      "Computing precisions and recalls, class 8/20\n",
      "Computing precisions and recalls, class 9/20\n",
      "Computing precisions and recalls, class 10/20\n",
      "Computing precisions and recalls, class 11/20\n",
      "Computing precisions and recalls, class 12/20\n",
      "Computing precisions and recalls, class 13/20\n",
      "Computing precisions and recalls, class 14/20\n",
      "Computing precisions and recalls, class 15/20\n",
      "Computing precisions and recalls, class 16/20\n",
      "Computing precisions and recalls, class 17/20\n",
      "Computing precisions and recalls, class 18/20\n",
      "Computing precisions and recalls, class 19/20\n",
      "Computing precisions and recalls, class 20/20\n",
      "Computing average precision, class 1/20\n",
      "Computing average precision, class 2/20\n",
      "Computing average precision, class 3/20\n",
      "Computing average precision, class 4/20\n",
      "Computing average precision, class 5/20\n",
      "Computing average precision, class 6/20\n",
      "Computing average precision, class 7/20\n",
      "Computing average precision, class 8/20\n",
      "Computing average precision, class 9/20\n",
      "Computing average precision, class 10/20\n",
      "Computing average precision, class 11/20\n",
      "Computing average precision, class 12/20\n",
      "Computing average precision, class 13/20\n",
      "Computing average precision, class 14/20\n",
      "Computing average precision, class 15/20\n",
      "Computing average precision, class 16/20\n",
      "Computing average precision, class 17/20\n",
      "Computing average precision, class 18/20\n",
      "Computing average precision, class 19/20\n",
      "Computing average precision, class 20/20\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(model=model,\n",
    "                      n_classes=n_classes,\n",
    "                      data_generator=dataset,\n",
    "                      model_mode=model_mode)\n",
    "\n",
    "results = evaluator(img_height=img_height,\n",
    "                    img_width=img_width,\n",
    "                    batch_size=8,\n",
    "                    data_generator_mode='resize',\n",
    "                    round_confidences=False,\n",
    "                    matching_iou_threshold=0.5,\n",
    "                    border_pixels='include',\n",
    "                    sorting_algorithm='quicksort',\n",
    "                    average_precision_mode='sample',\n",
    "                    num_recall_points=11,\n",
    "                    ignore_neutral_boxes=True,\n",
    "                    return_precisions=True,\n",
    "                    return_recalls=True,\n",
    "                    return_average_precisions=True,\n",
    "                    verbose=True)\n",
    "\n",
    "mean_average_precision, average_precisions, precisions, recalls = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane     AP    0.713\n",
      "bicycle       AP    0.722\n",
      "bird          AP    0.624\n",
      "boat          AP    0.588\n",
      "bottle        AP    0.341\n",
      "bus           AP    0.716\n",
      "car           AP    0.72\n",
      "cat           AP    0.809\n",
      "chair         AP    0.488\n",
      "cow           AP    0.707\n",
      "diningtable   AP    0.678\n",
      "dog           AP    0.793\n",
      "horse         AP    0.809\n",
      "motorbike     AP    0.718\n",
      "person        AP    0.617\n",
      "pottedplant   AP    0.326\n",
      "sheep         AP    0.625\n",
      "sofa          AP    0.75\n",
      "train         AP    0.798\n",
      "tvmonitor     AP    0.622\n",
      "\n",
      "              mAP   0.658\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(average_precisions)):\n",
    "    print(\"{:<14}{:<6}{}\".format(classes[i], 'AP', round(average_precisions[i], 3)))\n",
    "print()\n",
    "print(\"{:<14}{:<6}{}\".format('','mAP', round(mean_average_precision, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
